{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73ab4160-ac00-4235-9341-ff813681f712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Part 3:  Geospatial Join and Visualization Tutorial #\n",
    "This is a tutorial to show you how to join your geospatial data to polygon geometry and visualize it in Carto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f58d52ed-9741-4ddd-b6b9-a0bae85a14d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "start with a geospatial-enabled cluster with Apache Sedona installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f594991e-86c9-44e0-8ef7-1c128fb5aac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#If Sedona is not installed, like a serverless cluster, you may enable it like this.\n",
    "spark.conf.set(\"spark.geo.st.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cd9f903-e94b-4df1-ae0a-adf0b5a89ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the upper right hand corner, select \"Connect\" and select \"geospatial-cluster-smallest-available\". It may take a few minutes to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d05d300-d875-4e44-aff4-4dcc9770c908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## import tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d4c6463-1324-42c3-9d91-6143aad43b2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6721a0dd-71e5-431d-93e3-8457a6c55b26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After installing tools, restart Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c459b16-5ed3-45cd-8858-7cf7ffea7b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de688123-a4b7-4fa2-b467-7e2595e55c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "import sedona\n",
    "import geopandas as gpd\n",
    "from sedona.sql.types import GeometryType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5424eb19-c54c-4d3b-93b6-4ea797311307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bb0361c-de2f-432d-86e4-4d86933f9aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5030d53e-9c1b-45c2-9024-2939f574fa29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Go to the catalog to find the path to your data set. They are stored as Delta Tables, a variant of Parquety which are light-weight, static files on S3, which is easy to query and has version control features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c57d2c60-4446-47eb-a6c8-073b25587b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## read the table as a Spark Dataframe ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad70020e-e1c9-4ee7-99cf-eb420c76f754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sdf = spark.read.table(\"moo_ops_workspace.threeoneone.sr_mirror_2024\")\n",
    "sdf.printSchema()\n",
    "sdf.display(5)\n",
    "\n",
    "#reduce the count of files to speed it up\n",
    "sdf = sdf.where(col(\"CREATED_DATE\") > '2024-09-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87e77e48-0d08-4742-bfe4-707f6e659424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Parse the Lat Lon into a point##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7eddaf4-0127-4495-85f4-0dce1d272859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Carto stores everything as a WKB or Well-Known Binary. To join two geometries they need to both be in the same WKB Format. \n",
    "\n",
    "You may convert to WKB using ST_AsBinary(). This is called serialization. This converts it into a from that Carto can read.\n",
    "\n",
    "You may reverse this by ST_GeomFromWKB. This is called deserialization. This converts it into a format that Databricks can read.\n",
    "\n",
    "'ST' Either stands for Spatial Type or 'Spatial Transformation', not sure. When you cast the Latitudes and Longitudes using ST_Point, your Spark Dataframe becomes a Sedona Geospatial dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fb05f5f-c658-4d01-bf16-6c280ef58d20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b61552-b0ef-48cf-8eeb-dd3d0b4d02d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#turn your spark dataframe into a sedona geodataframe by creating an ST_POINT colum.\n",
    "points_311 = sdf.withColumn(\"the_geom\", expr(f\"ST_AsBinary(ST_Point(LON, LAT))\"))\n",
    "points_311 = points_311.withColumn(\"geom_deserialized\", expr(f\"ST_AsWKT(ST_GeomFromWKB(the_geom))\"))\n",
    "points_311.display(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b357345-35e4-4bba-a3c5-2c7fa4d0d19f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import a geospatial dataset ##\n",
    "## In this step we import a Carto-Prepared geometry. ##\n",
    "Carto geometry needs to have x and y max and mins for each geometry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "401e91af-1e2f-47cf-bf54-7a52716ac1ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b6c4ec1-855e-444e-9eeb-c1f28750b50e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When you attempt to import a map into Carto, it will throw an error and ask you if you want to prepare it. A *Prepared map* adds the carto max and min bounding box to the dataframe.\n",
    "\n",
    "For future reference, to prepare your geometry manually, you may add the following columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa1afe35-f74f-43f6-9155-07f68fa3b63e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Convert a Shapefile into a Sedona dataframe #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1356aadb-46c0-41e0-891a-3ec27e278904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shapefile_filepath = \"/Volumes/moo_ops_workspace/geospatial/geospatial_files_volume/BusinessImprovementDistrict.zip\"\n",
    "#if you import a shapefile, you have to specify the schema. We will use a geojson instead.\n",
    "filepath = \"/Volumes/moo_ops_workspace/geospatial/geospatial_files_volume/BusinessImprovementDistrict.geojson\"\n",
    "filepath_no_crs = \"/Volumes/moo_ops_workspace/geospatial/geospatial_files_volume/BusinessImprovementDistrict_no_crs.geojson\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d52921f4-1e0f-416b-98d4-c168e8abdf80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#reads the shapefile into a Pandas Geodataframe\n",
    "gdf = gpd.read_file(filepath)\n",
    "gdf.info()\n",
    "gdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805af340-17f4-4bd0-9c70-62b531362c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#read the shapefile as spark dataframe\n",
    "\n",
    "#bid_map = spark.read.format(\"shapefile\").load(shapefile_filepath)\n",
    "bid_gdf = gpd.read_file(filepath_no_crs)\n",
    "#convert geodataframe to pandas dataframe by deserializing the geometry\n",
    "bid_gdf['geom_deserialized'] = bid_gdf['geometry'].apply(lambda x: x.wkt)\n",
    "#delete the geometry, the binary will cause errors and won't load.\n",
    "bid_gdf = bid_gdf.drop(columns=['geometry'])\n",
    "#bid_gdf['geom_deserialized'] = bid_gdf['geometry'].apply(lambda x: str(x))\n",
    "bid_gdf.head(5)\n",
    "#create a spark dataframe. This is WKT, not a spatial dataframe\n",
    "bid_map = spark.createDataFrame(bid_gdf)\n",
    "# Add carto max and min bounding box columns\n",
    "bid_map = bid_map.withColumn(\"__carto_xmin\", expr(\"ST_XMin(geom_deserialized)\"))\n",
    "bid_map = bid_map.withColumn(\"__carto_xmax\", expr(\"ST_XMax(geom_deserialized)\"))\n",
    "bid_map = bid_map.withColumn(\"__carto_ymin\", expr(\"ST_YMin(geom_deserialized)\"))\n",
    "bid_map = bid_map.withColumn(\"__carto_ymax\", expr(\"ST_YMax(geom_deserialized)\"))\n",
    "display(bid_map)\n",
    "\n",
    "#DO NOT convert to a spatial dataframe. You will no longer be able to display it or get the schema.You also can't upload it as a table.\n",
    "#bid_map = bid_map.withColumn(\"geometry\", expr(f\"ST_GeomFromWKT(geom_deserialized)\"))\n",
    "#this will fail\n",
    "#display(bid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5d4485b-5253-4893-96a9-886f39e3fe86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e3f7229-002a-4977-890f-4ad0db46e712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Optional: Partition the table #\n",
    "This will help speed up queries when you have multiple workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be47523-3ec0-447c-8949-c11758c9226c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n",
    "points_311 = points_311.repartition(200)\n",
    "#bid_map = bid_map.repartition(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd5f9c84-4b4a-41d3-aaaa-9e03297d02d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Join the two maps using geospatial SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84712399-f1e1-4b60-96de-28a26281afbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "the point geometry will be suprerceded with the polygon geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8fd816e-11f3-46fd-957e-5cfb8e054486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pass the data frames in as variables in an python f-string\n",
    "points_311.createOrReplaceTempView(\"points_311\")\n",
    "spark.catalog.dropTempView(\"bid_map\")\n",
    "bid_map.createOrReplaceTempView(\"bid_map\")\n",
    "#group by all the geometry columns to keep them.\n",
    "#In order to keep the map carto-prepared you need to preserve the __carto_ columns.\n",
    "result_df = spark.sql(f\"\"\"SELECT\n",
    "                        FIRST(__carto_xmin) AS __carto_xmin,\n",
    "                        FIRST(__carto_xmax) AS __carto_xmax, \n",
    "                        FIRST(__carto_ymin) AS __carto_ymin, \n",
    "                        FIRST(__carto_ymax) AS __carto_ymax, \n",
    "                        COUNT(*) as 311_calls, \n",
    "                        bid_map.BIDID,\n",
    "                        bid_map.BID,\n",
    "                        FIRST(bid_map.geom_deserialized) AS geom_deserialized\n",
    "                        \n",
    "                      FROM points_311, bid_map\n",
    "                      --ST_Contains is where the spatial join happens. It is done on the deserialized columns,\n",
    "                      -- i.e. a Spark Dataframe, not a Spatial Dataframe\n",
    "                      WHERE ST_Contains(bid_map.geom_deserialized, points_311.geom_deserialized)\n",
    "                      AND points_311.created_date >= '2024-11-01'\n",
    "                      GROUP BY  bid_map.BIDID, bid_map.BID\n",
    "                      \"\"\"\n",
    "                      )\n",
    "result_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85afa060-0fd8-42d6-bba0-dedfaeead764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Save the Map#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66619fd0-fa28-46d8-bb50-ea89742ca3f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS moo_ops_workspace.threeoneone.311_calls_by_bid_test_from_databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0f713bb-28cc-4347-b36d-8fa0f03cb0e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#save result_df as a geospatial delta table in the catalog\n",
    "result_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"moo_ops_workspace.threeoneone.311_calls_by_bid_test_from_databricks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "715bb89a-cffd-465a-8096-19f76f45c635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#save as a non-geospatial table\n",
    "no_map_df = result_df[['311_calls', 'BID', 'BIDID']]\n",
    "no_map_df.write.format('delta').mode(\"overwrite\").saveAsTable(\"moo_ops_workspace.threeoneone.311_calls_by_bid_test_from_databricks_no_map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7d30e2b-949a-41e3-b782-c47cfdf1d56b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Visualization using Folium #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14501bfd-b748-4e71-8c50-a5d66fd53d76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Folium is widely used in industry for map visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "747b8c0e-e193-4c56-862a-c3ff4d1214ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#define color palate\n",
    "def get_color_rgb(calls):\n",
    "    if calls < 20000:\n",
    "        return \"rgb(255, 255, 204)\"\n",
    "    elif calls < 40000:\n",
    "        return \"rgb(255, 237, 160)\"\n",
    "    elif calls < 60000:\n",
    "        return \"rgb(254, 217, 118)\"\n",
    "    elif calls < 80000:\n",
    "        return \"rgb(254, 178, 76)\"\n",
    "    else:\n",
    "        return \"rgb(253, 141, 60)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da74ed0e-e8b2-4fb9-9890-b20ba86873d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To visuallize the data, we need to convert from Spark to Pandas format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee6c22e5-d747-4bbd-8207-8103f9940602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# have to get out of spark dataframe and into a geopandas geodataframe.\n",
    "df = result_df.toPandas()\n",
    "#load the geometry into a geopandas dataframe\n",
    "df['geometry'] = df['geom_deserialized']\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry', crs='EPSG:4326')\n",
    "gdf.head(5)\n",
    "#apply a fill color for the data\n",
    "gdf['fill_color_rgb'] = gdf['311_calls'].apply(get_color_rgb)\n",
    "del gdf['geom_deserialized']\n",
    "#simplify the geometry to make it smaller. Prevents the json recursion from getting too deep.\n",
    "gdf['geometry'] = gdf['geometry'].simplify(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e51b6637-d81d-4ffc-82fa-57db59fad103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc789350-301c-4822-8889-18f11f743fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gdf['fill_color_rgb'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6215f3b-19cc-433d-be0a-5310aff19b17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Convert GeoPandas Dataframe to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e46764d-8457-4150-aad2-92d846572802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "geojson = gdf.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96424890-c363-4c66-a2b6-26aacc2bd449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location=[40.7128, -74.0060], zoom_start=11)\n",
    "# folium takes color properties in the format \"rgb(255, 255, 255)\" or \"rgba(255, 255, 255, 1.0)\". It also accepts hex \"#ff0000\" for color\n",
    "folium.GeoJson(geojson, \n",
    "               style_function=lambda feature: { 'fillColor': feature['properties']['fill_color_rgb'],\n",
    "                                                'color': 'black', \n",
    "                                                'weight': 1, \n",
    "                                                'fillOpacity': 0.8\n",
    "                                             }           \n",
    "               ).add_to(m)\n",
    "display(m)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 144401040099489,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Part 3: 311 geospatial join tutorial for BIDs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
