{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c2e13a-bf21-4f7b-949e-1309c07fada8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#first, upload the file to the databricks file system (DBFS) in a Volume. This is manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6756168-8e6e-4bc1-8823-017848df9881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# converting a geospatial file to a topojson.\n",
    "\n",
    "## Why convert to a TopoJSON? ##\n",
    "Most geojson files are coordinate based. A topoJSON uses a series of arcs to store its geometry. Why use TopoJSON? Because PowerBI's Shape Map visulaization component only works with TopoJSON files.\n",
    "\n",
    "Note you could do this processing on your desktop computer as well, it might help you to save it locally.\n",
    "\n",
    "First, upload the file to the databricks file system (DBFS). This is manual.\n",
    "Second, start a cluster. We have clusters with geospatial tools installed and that would that would help you out. But serverless is faster and cheaper and you can use pip install to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c511ffc0-1e1b-4b01-84e1-cef56a094727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4611efd-e5f0-4e58-b4b1-0a0b33458b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install geopandas\n",
    "%pip install topojson\n",
    "%pip install geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85fea3bb-3c1c-4f63-8897-f6607f043310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "restart the kernal after installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9f27ef-8836-4864-a5c3-ecaa406da099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c9f0ef6-6688-40a9-9bc0-cc43c8305c52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import topojson as tp\n",
    "import geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28193983-f002-490e-b06c-dfcab562f989",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filepath = \"/Volumes/scorecard_fulcrum/scorecard_fulcrum_records/scorecard_fulcrum_volume/2020NTAMap.json\"\n",
    "filepath = \"/Volumes/scorecard_fulcrum/geo/scoreacard_geospatial_files/community_districts_simplified.geojson\"\n",
    "\n",
    "\n",
    "#you can even do shapefiles.\n",
    "filepath = \"/Volumes/scorecard_fulcrum/geo/scoreacard_geospatial_files/BusinessImprovementDistrict.zip\"\n",
    "filepath = '/Volumes/moo_ops_workspace/geospatial/geospatial_files_volume/nypd.geojson'\n",
    "filepath = \"/Volumes/scorecard_fulcrum/geo/scoreacard_geospatial_files/PavementEdge_DSNY_Districts_Sections.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec7a8492-43b1-4c19-b03a-337da8d85928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#read the file. It can be a geojson, json, shapefile, any geopandas-compatible map.\n",
    "gdf = gpd.read_file(filepath)\n",
    "gdf.info()\n",
    "#print(gdf['precinct'])\n",
    "\n",
    "#you may need to simplify it if it is too large and you get a \"not serializable error\". The 0.0001 is the tolarance in degrees.\n",
    "#0.0001 degrees is about 34 feet latitude, 27.8 feet longitude\n",
    "gdf['geometry'] = gdf.simplify(0.0001)\n",
    "#gdf.info()\n",
    "#convert GeoDataFrame to GeoJSON\n",
    "print(\"gdf simplified\")\n",
    "my_geojson = gdf.to_json()\n",
    "print(\"gdf jsonified\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bae07204-a2fc-4218-a016-cf14609e8407",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#convert GeoJSON to  (This takes a while)\n",
    "my_topojson = tp.Topology(my_geojson)\n",
    "print(my_topojson)\n",
    "print(\"successfully converted to topojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35a07829-d9c8-4133-b512-b8c86d0d9c17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Write the file to the volume #\n",
    "From there you may download it to your local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75961305-1ac5-44c6-a8d0-0752d7f0134a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename = filepath.split(\"/\")[-1].split('.')[0]\n",
    "directory = filepath.split(\"/\")[:-1]\n",
    "directory = \"/\".join(directory)\n",
    "print(directory)\n",
    "topojson_path = f\"{directory}/{filename}_topojson.json\"\n",
    "with open(topojson_path, 'w') as f:\n",
    "    f.write(my_topojson.to_json())\n",
    "\n",
    "regular_json_path = f\"{directory}/{filename}_regular.json\"\n",
    "with open(regular_json_path, 'w') as f:\n",
    "    f.write(my_geojson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f68d42-bd80-4f7c-b700-b060bd5a8af5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_topojson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df09f32c-2181-4948-8f44-b39fabbcec90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Write the TopoJSON as a Delta Table (doesn't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d461f0f3-f333-4b49-830c-70e6ad123107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_geojson.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "903d05d2-ecc3-4fbf-99e7-3784bc6bd64e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(topojson_path)\n",
    "df.show()\n",
    "destination_path = f\"/scorecard_fulcrum/geo/{filename}\"\n",
    "destination_table = f\"scorecard.geo.{filename}\"\n",
    "print(destination_path)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(destination_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba3c9ddf-d6f1-4202-9088-2aafcc4b6955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(regular_json_path)\n",
    "destination_path = f\"/scorecard_fulcrum/geo/{filename}_regular_json\"\n",
    "print(destination_path)\n",
    "destination_table = f\"scorecard.geo.{filename}_regular_json\"\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(destination_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d73d762b-3529-4172-a325-0227b6c2aed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#lets try a different way\n",
    "- convert geodataframe to a pandas dataframe\n",
    "- convert pandas df to a spark df\n",
    "- save spark df as delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d437a3d7-f01b-4484-b0a0-aee177cefcd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## to be read by carto, geometry must be in well-known binary format ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9df90136-d14b-47fe-8df8-9f6ccdcd76ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "034d631e-f711-4539-bf40-955b6f743c61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shapely import wkb\n",
    "gdf2 = gdf.copy()\n",
    "#ensure geometry column contains valid geometry objects\n",
    "gdf2['geometry'] = gdf2['geometry'].apply(lambda geom: geom if geom.is_valid else geom.buffer(0))\n",
    "\n",
    "#convert geometry column to WKB format\n",
    "gdf2['geometry'] = gdf2['geometry'].apply(lambda geom: wkb.dumps(geom))\n",
    "\n",
    "#convert GeoDataFrame to Pandas DataFrame\n",
    "pdf = pd.DataFrame(gdf2)\n",
    "\n",
    "#Convert Pandas Dataframe To Spark DataFrame\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "sdf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"scorecard_fulcrum.geo.{filename}_from_pandas_to_delta_wkb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e55260e-70eb-4c34-afb4-21b251c4c178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "916a13d7-798d-4623-920d-0267c26b964b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# hmm this geometry doesnt' appear valid in Carto"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Convert Shapefile to TopoJSON",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
