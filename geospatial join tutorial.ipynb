{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73ab4160-ac00-4235-9341-ff813681f712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Geospatial Join and Visualization Tutorial #\n",
    "This is a tutorial to show you how to join your geospatial data to polygon geometry and visualize it in Carto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ccb4123-720c-454d-bfb1-918c6c7dd069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark VS GeoPandas #\n",
    "Geopandas is a very easy format to work with. It just \"works\" without too much fuss. But when using Databricks, it is best to stay in in Spark. Like with Pandas which uses Geopandas for geospatial analysis, Spark uses Apache Sedona for geospatial analysis. Sedona is very performant and optimized for working in Lakehouses like Databricks Delta Tables. \n",
    "\n",
    "To turn any Spark DataFrame or sdf, simply read the column with a Sedona function. Sedona functions being with \"ST\". Such as ```ST_Point*()``` to read the column as a Point, or ```ST_GeomFromWKB()``` to read binary polygon geometry like the kind found in Carto. This will create a geometry column, but it will be in binary. You need to *deserialize* the binary into Well-Known-Text by calling ```ST_ASWKT()``` Spatial Joins must happen on deserialized data, but Carto needs to preserve the seriallized data as a well-known binaryto render on their platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f58d52ed-9741-4ddd-b6b9-a0bae85a14d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "start with a geospatial-enabled cluster with Apache Sedona installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cd9f903-e94b-4df1-ae0a-adf0b5a89ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the upper right hand corner, select \"Connect\" and select \"geospatial-cluster-smallest-available\". It may take a few minutes to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31225081-44af-4b43-a816-e1e1658a0327",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5424eb19-c54c-4d3b-93b6-4ea797311307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sedona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de688123-a4b7-4fa2-b467-7e2595e55c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5030d53e-9c1b-45c2-9024-2939f574fa29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Go to the catalog to find the path to your data set. They are stored as Delta Tables, a variant of Parquety which are light-weight, static files on S3, which is easy to query and has version control features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c57d2c60-4446-47eb-a6c8-073b25587b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## read the table ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad70020e-e1c9-4ee7-99cf-eb420c76f754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#this reads the 311 data as a vanilla spark dataframe. It has a latitude and lognitude coordinate\n",
    "sdf = spark.read.table(\"moo_ops_workspace.threeoneone.sr_mirror_2024\")\n",
    "sdf.printSchema()\n",
    "sdf.display(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d05d300-d875-4e44-aff4-4dcc9770c908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## import tools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87e77e48-0d08-4742-bfe4-707f6e659424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Parse the Lat Lon into a point#\n",
    "## this will turn it into a Sedona geospatial dataframe ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7eddaf4-0127-4495-85f4-0dce1d272859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Carto stores everything as a WKB or Well-Known Binary. To join two geometries in Spark/Sedona they need to both be deserialized into Well Known Text\n",
    "\n",
    "You may convert to WKB using ST_AsBinary(). This is called serialization. This converts it into a from that Carto can read.\n",
    "\n",
    "You may reverse this by ST_GeomFromWKB. This is called deserialization. This converts it into a format that Databricks can read.\n",
    "\n",
    "'ST' Either stands for Spatial Type or 'Spatial Transformation', not sure. When you cast the Latitudes and Longitudes using ST_Point, your Spark Dataframe becomes a Sedona Geospatial dataframe.\n",
    "\n",
    "When it is time to upload to carto, the columns need to be serialized again into a Well-Known-Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fb05f5f-c658-4d01-bf16-6c280ef58d20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b61552-b0ef-48cf-8eeb-dd3d0b4d02d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#turn your spark dataframe into a sedona geodataframe by creating an ST_POINT colum.\n",
    "points_311 = sdf.withColumn(\"the_geom\", expr(f\"ST_AsBinary(ST_Point(LON, LAT))\"))\n",
    "#geom deserializaed is a string, well-known text format.\n",
    "points_311 = points_311.withColumn(\"geom_deserialized\", expr(\"ST_ASWKT(ST_GeomFromWKB(the_geom))\"))\n",
    "points_311.display(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b357345-35e4-4bba-a3c5-2c7fa4d0d19f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Import a geospatial dataset #\n",
    "## In this step we import a Carto-Prepared geometry. ##\n",
    "Sedona is already registered in the geospatial clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b6c4ec1-855e-444e-9eeb-c1f28750b50e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When you attempt to import a map into Carto, it will throw an error and ask you if you want to prepare it. A *Prepared map* adds the Carto max and min bounding box to the dataframe. You may prepare a map in the Carto interface by uploading a map, attempting to display it, and there will be a warning about the map needs to be prepared, and asking you to prepare it. When you have prepared it, it will save as a table in your catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805af340-17f4-4bd0-9c70-62b531362c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nypd_map = spark.read.table(\"moo_ops_workspace.carto.nypd_prepared\")\n",
    "nypd_map.display()\n",
    "### read the pinary geometry from the Carto prepared table, simplify the geometry to ease processing time, and deserialize it.\n",
    "nypd_map = nypd_map.withColumn(\"geom_deserialized\", expr(f\"ST_ASWKT(ST_SIMPLIFY(ST_GeomFromWKB(the_geom), 0.001))\"))\n",
    "nypd_map.display(5)\n",
    "nypd_map.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd5f9c84-4b4a-41d3-aaaa-9e03297d02d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Join the two maps using geospatial SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84712399-f1e1-4b60-96de-28a26281afbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "the point geometry will be suprerceded with the polygon geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0febaaf3-deb6-4087-b5c3-7a9528cc7f43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To take advantage of multiple workers, partition the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3fb75ad-11c3-4156-84e4-e43c0c1b57da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n",
    "points_311 = points_311.repartition(200)\n",
    "nypd_map = nypd_map.repartition(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8fd816e-11f3-46fd-957e-5cfb8e054486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pass the data frames in as variables in an python f-string\n",
    "points_311.createOrReplaceTempView(\"points_311\")\n",
    "nypd_map.createOrReplaceTempView(\"nypd_map\")\n",
    "#group by all the geometry columns to keep them.\n",
    "#In order to keep the map carto-prepared you need to preserve the __carto_ columns.\n",
    "#the group by will separate into groups by the deserialized geometry. \n",
    "# filter by date because this takes a long time. 3 months about 2 minutes.\n",
    "# only seems to be using the minimum two workers.\n",
    "result_df = spark.sql(f\"\"\"SELECT\n",
    "                        FIRST(nypd_map.__carto_xmin) AS __carto_xmin,\n",
    "                        FIRST(nypd_map.__carto_xmax) AS __carto_xmax,\n",
    "                        FIRST(nypd_map.__carto_ymin) AS __carto_ymin, \n",
    "                        FIRST (nypd_map.__carto_ymax) AS __carto_ymax,\n",
    "                        COUNT(*) as 311_calls, \n",
    "                        FIRST(nypd_map.precinct) AS precinct,\n",
    "                        FIRST(nypd_map.the_geom) AS the_geom,\n",
    "                        nypd_map.geom_deserialized\n",
    "                        \n",
    "                      FROM points_311, nypd_map\n",
    "                      --ST_Contains is where the spatial join happens. It is joined on the deserialized columns.\n",
    "                      WHERE ST_Contains(nypd_map.geom_deserialized, points_311.geom_deserialized)\n",
    "                        AND points_311.created_date >= '2024-09-01'\n",
    "                      --include in the group by all the columns you want to keep.\n",
    "                      GROUP BY \n",
    "                        nypd_map.geom_deserialized\n",
    "                      \"\"\")\n",
    "result_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "543a84b6-e9b7-409c-92ee-bb758fb619e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Note: Carto requires the Geometry column to be called the_geom**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85afa060-0fd8-42d6-bba0-dedfaeead764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Save the Map#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "040cffef-c6f1-49fd-bd6b-824654fc8843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "By saving the map, this should refresh the carto map. Carto reads the geospatial delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2756fed-a5c1-406a-baaf-3395b2808649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66619fd0-fa28-46d8-bb50-ea89742ca3f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS moo_ops_workspace.threeoneone.311_calls_by_precinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0f713bb-28cc-4347-b36d-8fa0f03cb0e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#save result_df as a delta table in the catalog\n",
    "#this will scale up to use available workers.\n",
    "result_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"moo_ops_workspace.threeoneone.311_calls_by_precinct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "715bb89a-cffd-465a-8096-19f76f45c635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#save as a non-geospatial table\n",
    "no_map_df = result_df[['311_calls', 'precinct']]\n",
    "no_map_df.write.format('delta').mode(\"overwrite\").saveAsTable(\"moo_ops_workspace.threeoneone.311_calls_no_map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c12e541-1508-4b9b-943f-c0fd000624f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Visualization in PyDeck #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8838fa83-a948-4db9-9f11-ad4191056366",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Display using pyDeck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "951061ce-c11c-405d-998a-89b13f546af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This isn't working for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b75756-54ee-4af2-b13a-8a42994359d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to map 311_calls to colors\n",
    "def get_color(calls):\n",
    "    if calls < 20000:\n",
    "        return [255, 255, 204, 160]\n",
    "    elif calls < 40000:\n",
    "        return [255, 237, 160, 160]\n",
    "    elif calls < 60000:\n",
    "        return [254, 217, 118, 160]\n",
    "    elif calls < 80000:\n",
    "        return [254, 178, 76, 160]\n",
    "    else:\n",
    "        return [253, 141, 60, 160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b27676b1-c2f6-4cee-a74d-454c8e68322a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_color_rgb(calls):\n",
    "    if calls < 20000:\n",
    "        return \"rgb(255, 255, 204)\"\n",
    "    elif calls < 40000:\n",
    "        return \"rgb(255, 237, 160)\"\n",
    "    elif calls < 60000:\n",
    "        return \"rgb(254, 217, 118)\"\n",
    "    elif calls < 80000:\n",
    "        return \"rgb(254, 178, 76)\"\n",
    "    else:\n",
    "        return \"rgb(253, 141, 60)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b6d117b-0dec-435b-875f-7fb996352300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e6d2ead-bd97-4c35-82aa-2fa47a57c604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pydeck as pdk\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import sys\n",
    "sys.setrecursionlimit(100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69ecff65-3cdb-48ce-bd68-b23bab2bc7ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# have to get out of spark dataframe and into a geopandas geodataframe.\n",
    "df = result_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1ed8f4-44ef-417d-b9b7-c0a8e89c58a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#load the geometry into a geopandas dataframe\n",
    "df['geometry'] = df['the_geom'].apply(shapely.wkb.loads)\n",
    "#df['geometry'] = df['geom_deserialized']\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry', crs='EPSG:4326')\n",
    "gdf.head(5)\n",
    "#apply a fill color for the data\n",
    "gdf['fill_color'] = gdf['311_calls'].apply(get_color)\n",
    "gdf['fill_color_rgb'] = gdf['311_calls'].apply(get_color_rgb)\n",
    "del gdf['the_geom']\n",
    "del gdf['geom_deserialized']\n",
    "#simplify the geometry to make it smaller. Prevents the json recursion from getting too deep.\n",
    "gdf['geometry'] = gdf['geometry'].simplify(0.0001)\n",
    "geojson = gdf.to_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7566c60c-a179-4866-9354-913302e2300c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gdf.head(5)\n",
    "geojson[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13230ae6-85ad-46ef-b034-b70704ea0d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "*Deck doesn't seem to be working for me*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d77198b6-ba2c-4f29-a4df-c0113f4c40de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "LAND_COVER = [[-74.2654, 40.4964], [-73.7004, 40.4964], [-73.7004, 40.9156], [-74.2654, 40.9156]]\n",
    "geojson_layer = pdk.Layer(\n",
    "    \"GeoJsonLayer\",\n",
    "    geojson,\n",
    "    opacity=0.8,\n",
    "    stroked=True, \n",
    "    filled=True,\n",
    "    get_fill_color='properties.fill_color',\n",
    "    pickable=True)\n",
    "'''\n",
    "polygon_layer = pdk.Layer(\n",
    "    \"PolygonLayer\",\n",
    "    LAND_COVER,\n",
    "    get_polygon='-',\n",
    "    get_fill_color='[0, 0, 0, 20]',)\n",
    "'''\n",
    "view_state = pdk.ViewState(latitude=40.7128, longitude=-74.0060, zoom=11, bearing=0, pitch=0)\n",
    "\n",
    "r = pdk.Deck(layers=[geojson_layer], \n",
    "             initial_view_state=view_state)\n",
    "r.to_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7d30e2b-949a-41e3-b782-c47cfdf1d56b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Visualization using Folium #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14501bfd-b748-4e71-8c50-a5d66fd53d76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Was able to get this to work. Folium is widely used in industry for map visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e51b6637-d81d-4ffc-82fa-57db59fad103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc789350-301c-4822-8889-18f11f743fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gdf['fill_color_rgb'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e46764d-8457-4150-aad2-92d846572802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "geojson = gdf.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96424890-c363-4c66-a2b6-26aacc2bd449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location=[40.7128, -74.0060], zoom_start=11)\n",
    "# folium takes color properties in the format \"rgb(255, 255, 255)\" or \"rgba(255, 255, 255, 1.0)\". It also accepts hex \"#ff0000\" for color\n",
    "folium.GeoJson(geojson, \n",
    "               style_function=lambda feature: { 'fillColor': feature['properties']['fill_color_rgb'],\n",
    "                                                'color': 'black', \n",
    "                                                'weight': 1, \n",
    "                                                'fillOpacity': 0.8\n",
    "                                             }           \n",
    "               ).add_to(m)\n",
    "display(m)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6437835244474894,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "geospatial join tutorial",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
